Introduction
Nos objectifs :
D’après l’organisation mondiale de la santé, la déficience visuelle a de graves répercussions
sur la qualité de vie des populations adultes. Les adultes ayant une déficience visuelle peuvent
avoir des taux plus élevés de dépression, d’anxiété, et de suicide. Ce handicap peut contribuer
à l’isolement social, et à l’absence de soutien.
D’un point de vue sécuritaire en France, il y a 100 000 à 150 000 feux de signalisation, mais
seulement 35% (soit 45 000) sont équipés de dispositifs sonores pour aider les personnes
malvoyantes ou aveugles. Ces dispositifs permettent une traversée plus sûre en émettant un
signal sonore lors du passage piéton vert, mais ils peuvent être inefficaces si le bruit ambiant
est trop fort. Les feux sonores sont également plus coûteux que les feux classiques : leur prix
varie entre 25 000 et 40 000 euros, soit environ 113% de plus qu'un feu standard. Un feu avec
volume ajustable et système tactile ou vibrant, plus adapté, coûte entre 35 000 et 50 000
euros, soit environ 180% plus cher.
D’un point de vue financier, notre dispositif revient à moins de 500 euros, ce qui le rend très
intéressant et plus abordable, même si celui-ci ne permet d’équiper qu’une seule personne.
Notre objectif est donc l'intégration à un prix abordable de personnes atteintes d’un handicap
ou d’une incapacité visuelle. Ces lunettes pourraient améliorer considérablement la qualité de
vie des personnes malvoyantes en leur offrant une plus grande indépendance et une meilleure
intégration dans leur environnement quotidien, en plus d'une sécurité accrue.
Notre projet consiste à programmer un Raspberry Pi 5 et à utiliser le flux vidéo d’une caméra
pour interpréter l'environnement urbain. Nous nous sommes d’abord focalisés sur
l'identification des feux de signalisation et des passages piétons, pour une sécurité accrue des
personnes aveugles ou malvoyantes. Un signal sonore interprétant l'identification de ces
infrastructures leur sera transmis et leur permettra de mieux interagir en société en
encourageant le déplacement individuel dans des villes peu, voire pas adaptés, pour leur
handicap.

Définitions :
OpenCV (Open Source Computer Vision Library) est une bibliothèque
open-source spécialisée en vision par ordinateur et en apprentissage
automatique. Les projets open-source permettent à n’importe qui de les
utiliser, de les modifier et de les redistribuer librement. OpenCV peut
être utilisée pour le traitement d'images en temps réel, la détection
d'objets, la reconnaissance faciale, et bien d'autres applications. Dans
notre cas, nous l'avons utilisé pour repérer des objets bien particuliers :
les feux de signalisation, les passages piétons et les obstacles.

Le Raspberry Pi est un nano-ordinateur monocarte, c'est-à-dire un
ordinateur réduit à l'essentiel, tenant sur une seule carte électronique de la
taille d'une carte de crédit. Il fonctionne avec un processeur ARM, comme
ceux des smartphones, et possède des ports USB, HDMI et GPIO qui
permettent de le connecter à divers périphériques.

Son principal intérêt est de rendre l'informatique accessible à tous, à un coût réduit, tout en
encourageant l’apprentissage de la programmation et des systèmes informatiques. En effet
son poids léger fait de lui un nano-ordinateur facilement transportable, ce qui nous intéresse
grandement dans notre projet, pour pouvoir créer un modèle de lunette compact et léger, pour
une utilisation la plus pratique possible.

Questionnement :
A ce stade de notre réflexion, nous nous sommes posés de nombreuses questions :
- Est-ce que la détection sera fiable ?
- Que voulons-nous repérer grâce à ces lunettes ?
- La caméra aura-t-elle une portée assez large ?
- Est-ce que la caméra sera assez petite et légère pour tenir sur des lunettes ?
- Quel type de signal allons-nous transmettre à la personne (à travers l’oreillette) ?
- Comment réguler le flux d'information qui arrivera à l'oreillette ?
- La caméra arrivera-t-elle à capter les objets assez rapidement ?
- Faut-il rajouter un voyant aux lunettes pour prévenir qu'elles filment ( pour l’éthique) ?
- Sous quelles conditions les lunettes fonctionnent (luminosité, contraste des couleurs...) ?

Premières réflexions
Le projet consiste à interpréter l’environnement urbain en utilisant la caméra. Le flux vidéo est
ensuite envoyé au dispositif informatique autonome. Il faudra donc programmer la Raspberry
Pi 5 avec son accélérateur d'IA HAT pour interpréter les informations reçues en direct.

Lors de notre projet, nous nous sommes d’abord focalisés sur l’identification des feux de
signalisation et des passages piétons, car ce sont deux éléments non indiqués sur des
applications comme ‘maps’, mais qui sont pourtant essentiels à la sécurité des personnes
malvoyantes ou aveugles lors de leurs déplacements.
Bien que notre dispositif ne puisse pas empêcher tous les accidents, il
permettra de protéger au mieux la traversée des routes en milieu
urbain. Une fois que notre dispositif a identifié le passage protégé et/ou
le feux de signalisation qui est bien vert pour les piétons, comment
prévenir la personne ?
Nous avions pensé dans un premier temps utiliser un signal sonore. Mais il y a de fortes
chances que la personne utilise déjà des oreillettes pour recevoir les informations provenant
de l’application ‘Maps’ par exemple. Notre système ne doit donc pas interférer avec des
écouteurs. De plus, en ville, la pollution sonore et les bruits environnants peuvent atteindre
jusqu’à 110 dB ! Il nous paraissait donc impossible de générer un système sonore pouvant
pallier tous ces problèmes.

Nous avons alors décidé d’avertir l’utilisateur grâce à une vibration du boîtier, la Raspberry pi
5 étant compatible avec des systèmes de buzzer. Il nous reste ensuite la question du message
en lui-même, comment avertir l'utilisateur ? Nous avons plusieurs idées en tête qu’il nous
faudra tester avant de choisir la plus efficace. Pour l’instant la solution la plus simple nous
semble être l’émission de deux vibrations distinctes : une plus rapide et puissante pour avertir
d’un danger imminent et une autre plus faible et longue pour signaler que le passager peut
traverser.

Installation des outils de développement
Maintenant que le contexte est posé, nous allons commencer à exploiter le flux vidéo de la
caméra. Nous nous sommes documentés et nous avons été orientés pour utiliser une
bibliothèque d’outils spécialisée dans la vision : OpenCV.
Après une longue réflexion et quelques essais, nous avons opté pour une installation plus
globale, c’est-à-dire une installation de cette bibliothèque dans un environnement de travail
complet qui intègre tous les outils de développement dont nous aurons besoin : OpenCV +
Python + IDE pour python. Nous avons donc procédé à l’installation d’Anaconda.
Pour le moment, nous avons un ensemble opérationnel avec un environnement sous
Windows, mais à terme, nous pensons travailler avec Linux. Comme nous avons choisi de
l’informatique embarqué avec une bibliothèque et des logiciels libres de droit, autant utiliser
également un environnement aussi libre de droit comme Linux.
Sous Anaconda, nous obtenons l’interface graphique suivante :

Depuis cet intégrateur d’environnement nous avons accès à tous nos outils, et il suffit
d’utiliser l’IDE (Integrated Development Environment) Spyder.
Maintenant que nous avons choisi notre système
informatique et les différents outils que nous allons utiliser
pour effectuer le développement, il ne reste plus que
l'essentiel, à savoir, le développement des différents
programmes permettant d’aider ces personnes.

Détection des feux et des passages piétons
Au commencement de notre projet, nous n’avions pas les connaissances nécessaires au
développement des programmes souhaités sur OpenCV. Nous avons dû apprendre à utiliser
cette bibliothèque OpenCV, qui est très complexe et qui demande de la technicité quant à sa
mise en œuvre. Dans cette partie nous aborderons essentiellement la détection des
couleurs.

1 - Affichage du flux vidéo capturé par la caméra dans une fenêtre :

Ce programme permet seulement
d’afficher le flux vidéo de la caméra et de
l’afficher dans une fenêtre :
Après avoir mis en œuvre la technique
permettant d’acquérir des images, voici
quelques exemples qui nous ont fait
comprendre comment détecter une
couleur.

2 - Qu’est-ce qu’une image ?
Une image est un ensemble de points élémentaire appelé pixel. La dalle d'un écran peut
être représentée sous forme matricielle. Dans l’exemple ci-dessous, nous avons un écran
de 60x35 :

Pour être affiché sur un écran, chaque pixel doit posséder
un certain nombre de caractéristiques. Les écrans ont pour
la plupart besoin des paramètres : R (Rouge), V (Vert) et B
(Bleu). Donc lorsque nous lisons une image avec un
traitement informatique, nous obtenons une image au format
RVB (BGR en anglais).

(
h
t
t
(//nsimichelet91.github.io/snt/Theme1_Image_numerique/cours/)

De plus, chaque couleur est codée avec
une valeur de 0 à 255. Cela donne une
possibilité de 256x256x256 = 16
777 216 couleurs différentes pour
chaque pixel. Voici ci-dessous des
exemples de valeurs pour certaines
couleurs

(https://nsimichelet91.github.io/snt/Theme1_Image_numerique/cours/)
Le modèle RGB a été techniquement élaboré pour les différents écrans. Cependant, il existe
d’autres modèles de représentation des couleurs dans lesquels la luminosité ambiante
interfère nettement moins la détection de celle-ci. C’est notamment le cas pour le modèle
HSV.
Voici des exemples montrant la différentes entre ces deux modèles :

Image codée en RGB :

Image codée en HSV :

Comme vous pouvez le voir, le modèle HSV a pu mettre clairement en évidence les limites
du citron, en plus, elle a été capable d'éliminer les effets d'éclairage sur le citron lui-même. Il
en est de même avec l’image du portrait ou des zones de couleurs apparaît. Mais pour cette
image ou la palette des couleurs est moins vaste, le résultat obtenu doit être un peu plus
interprété.
Une autre observation est qu'il y a une rougeur autour du citron, pour en comprendre la
raison, il faut savoir que pour le modèle HSV, il n’y a que des couleurs, pas de teintes
blanche/gris/noir, ce qui signifie que si l'image a une partie qui est incolore (blanc, gris, noir),
alors sa représentation en HSV sera presque aléatoire, parce que la saturation et la valeur
vont détruire la couleur résultant de la couleur grisâtre que nous voyons.
Que signifie l'acronyme HSV ?

H (Hue) : représente les différentes couleurs disponibles et nous pouvons accéder à
différentes couleurs en donnant des valeurs différentes pour la teinte sous forme d'angle.

S (Saturation) : représente la quantité de couleur/pigment. Cela indique par exemple, que
pour une même couleur, l'objet est représenté avec différente nuance en fonction de la
lumière ambiante.
V (Valeur) : représente la valeur de la brillance de la couleur. Cela indique par exemple, que
pour une même couleur, l'objet est représenté avec différente intensité en fonction de la
lumière ambiante.

C’est donc le modèle HSV qui a été choisi pour la recherche d’une couleur sur une image. Le
jeu d’instruction nécessaire est la suite : cv2.cvtColor (converti le modèle d’une couleur en un
autre modèle de couleur). Une mise œuvre simple serait la suivante :

Nous allons maintenant voir comment cela a été utilisé dans nos différents essais.

3 - Détection d’une seule couleur : ici le jaune
Nous avons commencé par essayer d’adapter des applications nous permettant, par la suite,
de détecter au mieux les couleurs qui nous intéressent. A savoir le rouge, l’orange et le vert.
Le programme ci-dessous permet d’identifier dans le flux vidéo la couleur jaune

4 - Premier essai de détection de trois couleurs :

Les résultats sont encore à affiner :

5 - Technique pour détecter un objet
La segmentation d'image est un processus crucial en vision par ordinateur qui consiste à
diviser une image en plusieurs segments ou régions. Cette technique aide à simplifier la
représentation d'une image, facilitant ainsi son analyse. OpenCV propose diverses méthodes
pour la segmentation d'image. Nous allons juste présenter les grands principes efficaces
pour la segmentation d'image qu'utilise OpenCV, sans entrer dans les détails
mathématiques.

a) - Le seuillage est l'une des techniques les plus simples et les plus couramment utilisées
pour la segmentation d'image. Il convertit une image en niveaux de gris en une image binaire,
où les pixels sont classés comme avant-plan ou arrière-plan en fonction d'une valeur seuil.
b) - La détection de contours est une autre technique efficace pour la segmentation d'image.
Elle identifie les limites des objets dans une image, permettant une segmentation précise.

c) - Un algorithme de segmentation avancé comme GrabCut utilise des coupes de graphe

pour séparer l'avant-plan de l'arrière-plan. Il nécessite un rectangle englobant autour de
l'objet d'intérêt.
La liste des algorithmes et des méthodes est assez longue.

6 - Premier essai pour la détection d’objet : un passage piéton
Le programme ci-après permet de détecter les formes caractéristiques des contours blancs
des passages piétons.

Nous avons indiqué ci-dessous deux exemples d’images contenant un passage pour piéton :
Avant analyse de l’image :

Image après analyse :

Nous constatons qu’en utilisant la bibliothèque OpenCV, il est possible d'obtenir un
ensemble de résultats. Aussi bien au niveau de la reconnaissance des couleurs qu’au niveau
de la détection d'objets particuliers, comme l’ont montré nos essais pour détecter un passage
piéton. Mais nous ne pouvons ignorer le nombre conséquent d’erreurs !

Au niveau des couleurs, la détermination précise des zones de couleurs n’est pas évidente.
Si la zone de couleur détectée est trop restreinte, celle- ci ne sera pas détectée. Et si nous
élargissons cette zone alors trop de zones de cette couleur seront détectées. Comme
précédemment la couleur “rouge”, qui est perçue a tellement d’endroit de l’image, que cela
devient inexploitable. Concernant la détection des passages piéton, soit nous détectons avec
ce programme une partie du passage piéton, soit pas du tout et le passage pour piéton est
perçu comme le rond-point.
Nous avons alors cherché un peu d’aide extérieur, et nous avons
réussi à prendre contact avec Judy, une doctorante dans sa
deuxième année à l'université de Lorraine. Judy a un diplôme
d'informatique, et travaille durant sa thèse sur les thèmes de la
robotique, de l'informatique et de l'IA : des sujets qui ont un lien
direct avec notre projet. C'est notamment elle qui, après notre
premier contact en visioconférence, nous a orientés sur le
modèle Ultralytics YOLO.
C’est un modèle de détection d'objets et de segmentation d'images en temps réel qui
s'appuie sur des avancées de pointe en matière d'apprentissage profond et de vision par
ordinateur. Et cela offre alors des performances inégalées en termes de rapidité et de
précision par rapport à d'autres modèles comme la bibliothèque OpenCV. Nous nous
sommes alors lancés sur cette nouvelle piste avec passion.

Le modèle Ultralytics YOLO
1 - Détection d’objet avec YOLO.
Maintenant nous allons prendre comme exemple une application YOLO qui détecte les
joueurs et les ballons de football à partir d'une image donnée.

https://www.datacamp.com/fr/blog/yolo-object-detection-explained
L’algorithme fonctionne selon les quatre approches suivantes : blocs résiduels / régression par boîte
englobante / intersection Over Unions ou IOU en abrégé / suppression non maximale

a) - Blocs résiduels :
Cette première étape commence par la division de l'image originale (A) en NxN cellules de
grille de forme égale, où N, dans notre cas, est 4, comme le montre l'image de droite. Chaque
cellule de la grille est chargée de localiser et de prédire la classe de l'objet qu'elle recouvre,
ainsi que la valeur de probabilité/confiance.

b) - Régression par boîte englobante :
L'étape suivante consiste à déterminer les boîtes de délimitation correspondant aux
rectangles, en mettant en évidence tous les objets de l'image. Il peut y avoir autant de
boîtes englobantes qu'il y a d'objets dans une image donnée.
YOLO détermine les attributs de ces boîtes de délimitation à l'aide d'un seul module
de régression dans le format suivant, où Y est la représentation vectorielle finale de
chaque boîte de délimitation.
Y = [pc, bx, by, bh, bw, c1, c2]
pc correspond au score de probabilité de la grille contenant un objet.
bx, by sont les coordonnées x et y du centre de la boîte englobante par rapport
à la cellule de la grille enveloppante.
bhbw correspondent à la hauteur et à la largeur de la boîte de
délimitation par rapport à la maille enveloppante. c1 et c2 correspondent aux deux
classes Joueur et Balle. Nous pouvons avoir autant de classes que votre cas

d'utilisation l'exige. Regardons de plus près le joueur en bas à droite :

c) - Intersection Over Unions ou IOU en abrégé :
La plupart du temps, un seul objet dans une image peut avoir plusieurs boîtes à grille
candidates à la prédiction, même si toutes ne sont pas pertinentes. L'objectif de l'IOU (une
valeur entre 0 et 1) est d'écarter ces cases de la grille pour ne garder que celles qui sont
pertinentes. En voici la logique :
- L'utilisateur définit son seuil de sélection des reconnaissances de dettes, qui peut
être, par exemple, de 0,5.
- Ensuite, YOLO calcule l'IOU de chaque cellule de la grille, qui correspond à la
zone d'intersection divisée par la zone d'union.
- Enfin, il ignore la prédiction des cellules de la grille ayant un IOU ≤ seuil et prend en
compte celles ayant un IOU > seuil

Nous avons ci-dessous une illustration de l'application du processus de sélection de la grille
à l'objet situé en bas à gauche. Nous pouvons observer que l'objet avait à l'origine deux
candidats à la grille, et que seule la "grille 2" a été sélectionnée à la fin.

d) - suppression non maximale.
La fixation d'un seuil pour la reconnaissance n'est pas toujours suffisante, car un objet
peut comporter plusieurs cases avec une reconnaissance supérieure au seuil, et le
fait de laisser toutes ces cases peut inclure du bruit. C'est ici que nous pouvons utiliser
la suppression non maximale pour ne conserver que les boîtes dont la probabilité de
détection est la plus élevée.

1 - Premier contact avec YOLO et les images
Nous avons réussi à implanter ce modèle sous Windows en passant par Anaconda.
Cependant cela pose encore un problème sous Linux.
Donc, sous Windows, avec l’intégrateur Anaconda, nous avons bien tous nos
différents outils qui sont exploitables. Le programme ci-dessous utilise le modèle
YOLO pour extraire des objets de l’image :

Image avant analyse :

Image après analyse :

Nous constatons très vite la puissance de ce modèle. Il possède une bibliothèque
d'objets de façon native, et celle-ci peut être étendue. De plus, il indique un degré de
pertinence pour chaque objet détecté. Il nous reste à faire apprendre à ce modèle la
reconnaissance des objets que l’on désire : les feux de signalisation et les passages
piétons.
Mais quand est-il pour le traitement d’un flux vidéo ?

2 - Deuxième approche avec cette fois un flux vidéo
Pour exploiter au mieux le flux vidéo, nous allons utiliser aussi bien la bibliothèque
OpenCV que le modèle YOLO :

Dans un premier temps, nous avons fait des essais pour réaliser le même type de
détection que sur une simple image. Mais, ci-dessous, la capture d’écran était sur le
flux vidéo de la caméra de l’ordinateur. Nous avons été bluffées par la puissance et la
rapidité de l’analyse réalisée, même si ce n’était que la détection d’individus (notre
équipe), et pas encore les objets voulus.

Puis nous avons commencé les premiers tests avec la capture vidéo d’un
environnement extérieur. Nous pouvons constater que le passage piéton n’est pas
encore identifié. En effet, les voitures sont bien repérées, mais le passage piéton, qui
n’est obstrué par aucun obstacle, et qui est bien en vue, n’est absolument pas repéré.

Design et support du dispositif
Notre première idée a été de fixer la caméra directement sur des lunettes. Cependant,
d’un point de vue logistique, nous avons dû repenser le design de notre projet puisque
la taille de la caméra empêchait un montage efficace et durable. En effet, la plupart
des caméras USB ont un conducteur électrique situé à l'arrière de la caméra. Par
conséquent, qu’il s’agisse de la taille, de la forme ou du poids de la caméra, rien ne
permettait de l’associer au design fin et élégant de lunettes.
Viens donc une des premières étapes de notre
questionnement : le support. Quel support
utiliser pour assurer la prise d’un maximum
d 'informations tout en restant utile et peu
encombrant pour l’utilisateur ?
Notre attention s’est tout d’abord portée sur
l’idée d’un bracelet ou d’un système se basant
sur le Bluetooth. Cependant nous savions que
certaines personnes malvoyantes ou aveugles
utilisent déjà des systèmes d’aides ou de
directions (tel qu’un GPS) et nécessite donc
l’utilisation d'écouteurs.

Ne pouvant donc pas perturber la communication filaire
ou Bluetooth nous avons trouvé l’idée de lunettes avec
haut-parleurs intégrés, qui seraient bien plus pratiques
qu’un buzzer, car plus compacts et plus un moyen de
communication. En effet, les lunettes sont déjà portées
par la plupart des personnes aveugles et malvoyantes,
en raison de l’intensité lumineuse, mais serviraient en
plus, dans le cadre de notre projet, à communiquer les
informations, et cela sans rajouter de boitiers, de
ceintures ou de câblages supplémentaires !

Partenariat
Nous avions à cœur de faire un partenariat avec une association pour personnes
malvoyantes et aveugles. Le cœur même de notre projet étant le bien-être et l'intégration de
ces personnes atteintes de déficience visuelle, nous voulions avoir l'avis et les conseils des
personnes concernées. Dès le début de notre projet, nous avons donc contacté plusieurs
fois l'association Voir Ensemble, localisée en Moselle, et nous avons pu avoir un partenariat
avec l’Auxiliaire des Aveugles de Moselle ! Nous assistons donc à leurs réunions pour
entendre leurs avis et leurs conseils surtout du point de vue design et pratique, pour que
notre projet final puisse les aider aux mieux dans leur vie quotidienne.

Evolution du projet.
Ce projet est actuellement en constante évolution. Tant le niveau de technicité
requis est important pour le traitement du flux vidéo. A SUIVRE!

Conclusion
Pour conclure, à l’aide de ce travail d’équipe, nous pouvons dire qu’après avoir fait
face à de nombreux défis et avoir effectué plusieurs changements imprévus, mais
nécessaire, nous avons réussi à avoir les résultats attendus. Ce projet nous a permis
d’en apprendre plus sur le travail d’équipe, la recherche, l’informatique, l’organisation
et la concrétisation d'idées ! Notre programme arrive à repérer, avec une marge
d’erreur raisonnable, les passages piétons et les feux de signalisation, ce qui pourrait
donc assurer la sécurité des personnes malvoyantes ou aveugles au sein des villes.
Nous avons encore bien sûr des détails à améliorer, d’un point de vue concret par
rapport à la taille de la caméra sur le bandeau, ou de la luminosité de
l’environnement… Mais avec une identification fiable et l’avancée des outils
technologiques, nous aimerions la concrétisation de ce projet : un outil qui vient
compléter les systèmes déjà existants, en offrant une sécurité supplémentaire, qui
pourrait être utilisé durablement !

Remerciements
Mr. Pierre, professeur du Club Robotique qui nous a aidé dans toutes nos initiatives.
Mr. Nivoix, professeur de NSI de notre lycée qui n’a pas hésité à nous guider
Judy AKL, doctorante de l’université de Lorraine qui a pris de son temps pour nous guider
dans nos idées.

Sources

https://nsimichelet91.github.io/snt/Theme1_Image_numerique/cours/
https://www.datacamp.com/fr/tutorial/installing-anaconda-windows
https://stacklima.com/configurer-opencv-avec-lenvironnement-anaconda/
https://learn.microsoft.com/fr-fr/windows/ai/windows-ml/tutorials/
https://www.anaconda.com/
https://opencv.org/
https://docs.ultralytics.com/fr
“Traitement d'images et de vidéos avec OpenCV 4 en Python” de Laurent
Berger Éditions D-BookeR

